<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="transformer generated music interpretation">
  <meta property="og:title" content="Auto-Dynamics: Transformer-Generated Interpretations of Piano Music"/>
  <meta property="og:description" content="transformer generated music interpretation"/>
  <meta property="og:url" content="https://grace4x.github.io/auto-dynamics-site/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="Auto-Dynamics: Transformer-Generated Interpretations of Piano Music">
  <meta name="twitter:description" content="transformer generated music interpretation">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="transformer generated music interpretation">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Auto-Dynamics</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Auto-Dynamics: Transformer-Generated Interpretations of Piano Music </h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://github.com/grace4x" target="_blank">Grace Xu</a></span>
                  </div>

                  <div class="is-size-5 publication-authors">
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="static/pdfs/auto-dynamics.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/grace4x/auto-dynamics" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            In the past decade, there have been large advancements in the field of music processing due to the application of
neural networks to classic tasks such as music generation, transcription, and classification. This work concentrates
rather on generating musical interpretation, which is a set of custom changes the performer may add to a piece of
music during performance to convey personal expression. Specifically, we focus on auto-generating dynamics, or
custom volumes, for a given series of notes. We evaluate our dynamics model in five categories and demonstrate
that the Transformer architecture works effectively in generating artistically expressive performances. These
findings suggest that the Transformer has potential to excel in subjective areas such as artistic style.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="section hero is-light"></section>
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Examples</h2>
        <div class="content has-text-justified">
          <table>
            <tr>
              <th center>Title of Piece</th>
              <th center>No Dynamics</th>
              <th center>Generated Dynamics</th>
            </tr>


            <tr>
              <td>Chopin: Nocturne Op. 27 No. 1 in C-sharp minor</td>
              <td>
                <audio controls=""> <source src="static/audio/chopin-new.mp3" type="audio/mpeg" /> </audio>
              </td>
              <td>
                <audio controls=""> <source src="static/audio/chopin-new-nd.mp3" type="audio/mpeg" /> </audio>
              </td>
            </tr>

            <tr>
              <td>Liszt: Concert Etude No. 2, "Gnomenreigen," S. 145</td>
              <td>
                <audio controls=""> <source src="static/audio/liszt-new.mp3" type="audio/mpeg" /> </audio>
              </td>
              <td>
                <audio controls=""> <source src="static/audio/liszt-new-nd.mp3" type="audio/mpeg" /> </audio>
              </td>
            </tr>

            <tr>
              <td>Debussy: Estampes No. 3, "Jardins sous la Pluie," L. 100</td>
              <td>
                <audio controls=""> <source src="static/audio/debussy-new.mp3" type="audio/mpeg" /> </audio>
              </td>
              <td>
                <audio controls=""> <source src="static/audio/debussy-new-nd.mp3" type="audio/mpeg" /> </audio>
              </td>
            </tr>

            <tr>
              <td>Beethoven: Sonata No. 3 in C Major, Op. 2, I. Allegro con brio</td>
              <td>
                <audio controls=""> <source src="static/audio/beethoven-new.mp3" type="audio/mpeg" /> </audio>
              </td>
              <td>
                <audio controls=""> <source src="static/audio/beethoven-new-nd.mp3" type="audio/mpeg" /> </audio>
              </td>
            </tr>

            <tr>
              <td>Bach: Prelude and Fugue in B-flat minor, BWV 891</td>
              <td>
                <audio controls=""> <source src="static/audio/bach-new.mp3" type="audio/mpeg" /> </audio>
              </td>
              <td>
                <audio controls=""> <source src="static/audio/bach-new-nd.mp3" type="audio/mpeg" /> </audio>
              </td>
            </tr>

          </table>

          <p>Some examples of score-generated midi files from the internet without expressive timing or pedal: </p>

          <table>
            <tr>
              <th center>Title</th>
              <th center>Original</th>
              <th center>Generated Dynamics</th>
            </tr>


            <tr>
              <td>Happy Birthday Variations (Arr. by Jonny May on Musescore)</td>
              <td>
                <audio controls=""> <source src="static/audio/birthday_orig.mp3" type="audio/mpeg" /> </audio>
              </td>
              <td>
                <audio controls=""> <source src="static/audio/birthday_inference.mp3" type="audio/mpeg" /> </audio>
              </td>
            </tr>

            <tr>
              <td>Yiruma: River Flows in You</td>
              <td>
                <audio controls=""> <source src="static/audio/yiruma_orig.mp3" type="audio/mpeg" /> </audio>
              </td>
              <td>
                <audio controls=""> <source src="static/audio/yiruma_inference.mp3" type="audio/mpeg" /> </audio>
              </td>
            </tr>

          </table>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <div class="content has-text-justified">
          <img src="static/images/auto-dynamics-method.png" alt="auto dynamics method"/>          
        </div>
        <div class="content has-text-justified is-four-fifths">
          <p>
            This project uses an encoder-decoder Transformer model to generate musical dynamics given a MIDI file. MIDI events are first converted
            into a time-by-pitch matrix, which is used as encoder input and then decoded into velocity generations. 
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Melody Recognition</h2>
        <div class="content has-text-justified">
          <img style="width:100%" src="static/images/melody-recog.png" alt="auto dynamics method"/>          
        </div>
        <div class="content has-text-justified is-four-fifths">
          <p>
            An excerpt from Chopin’s Nocturne Op. 27 No. 1 in c-sharp minor with dynamics as generated by the base model.       
          <p>
            In this excerpt, the model successfully differentiates between the melody line and the accompaniment. The melody can be seen as the set of horizontal lines highlighted blue in the top third of the image, and the accompaniment consists of the notes below the melody. The line of vertical bars at the bottom of the page correspond to the volumes of the notes, with each note having a matching vertical bar.
          <p>
            The generated velocities for this sample are mostly in the quiet 40-50 range, except for the melody line, which is brought out clearly and loudly (see the smattering of taller blue bars at the bottom of the page). This indicates a successful attempt at recognizing and prioritizing the melody.
        </div>
          </div>
    </div>
  </div>
</section>

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Voicing</h2>
        <div class="content has-text-justified">
          <img style="width:100%" src="static/images/voicing.png" alt="auto dynamics method"/>          
        </div>
        <div class="content has-text-justified is-four-fifths">
          <p>
            An excerpt from Rachmaninoff’s Elegie Op. 3 No. 1, with dynamics as generated by the base model.
          <p>
            In piano music, chords (three or more notes played at the same time) are generally “voiced,” meaning the most melodically important note within the chord is played louder than the others. Generally, the most important note is the top note.
          <p>
            The excerpt has three sections highlighted in different colors for convenience: the accompaniment is depicted in red, with a line of chords above it in green and blue. The blue depicts the most melodically important note, and the green depicts the rest of the chord which helps harmonize the melody.
          <p>
            The model distinguishes accurately between the three and treats them appropriately. The blue melody line has the tallest bars, indicating that it is correctly “voiced.” The accompaniment is represented by short red bars at the bottom, indicating that it is evenly quiet despite the large range of different pitches. The green bars are generally taller than the red and shorter than the blue.
        </div>
          </div>
    </div>
  </div>
</section>

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Phrasing</h2>
        <div class="content has-text-justified">
          <img style="width:100%" src="static/images/phrasing.png" alt="auto dynamics method"/>          
        </div>
        <div class="content has-text-justified is-four-fifths">
          <p>
          An excerpt from Mozart’s Sonata No. 15 in F Major, K533, with dynamics as generated by the base model.
          <p>
          Musical “phrasing” refers to how a sequence of notes may have different individual volumes to convey emotion and
          expression. A general rule of phrasing is that higher notes are played louder. In fact, “Director Musices” project [15]
          mentioned in the Related Work section had this rule as the first of twenty-four.
          In this excerpt, the mountain-shaped notes in the middle are successfully accompanied by slightly taller bars at the
          bottom.
          <p>
          Another rule of phrasing is that the same sequence of notes should not be repeated the same way twice. The notes
          highlighted in blue correspond with the notes highlighted in green, and the model successfully differentiates them by
          making the second set softer (the green bars are shorter than the blue bars)        </div>
          </div>
    </div>
  </div>
</section>

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Harmonic Understanding</h2>
        <div class="content has-text-justified">
          <img style="width:100%" src="static/images/harmonic-understanding.png" alt="auto dynamics method"/>          
        </div>
        <div class="content has-text-justified is-four-fifths">
          <p>
            An excerpt from Chopin’s Nocturne Op. 27 No. 2 in D-flat Major, with dynamics as generated by the base
            model.
          <p>
            As mentioned above, a general rule of phrasing is that higher notes are played louder. However, performers often
            choose to override this rule if it conflicts with other rules.
          <p>
            In this excerpt, the highest note of the melody (a Bb5) should theoretically be played the loudest. However, there is a
            conflicting harmonic rule which dictates that it should be played softer: everything before this note is firmly within
            the D-flat Major tonic chord, and this note not only ventures outside of the tonic, but is also in the minor key.
          <p>
            The model successfully overrides the “higher notes are louder” rule to play this note softly, which demonstrates that
            it has some level of harmonic understanding. It is likely that this is not a fluke, since the previous melody notes (all
            highlighted in blue) follow the “higher notes are louder” rule.
          </div>
      </div>
    </div>
  </div>
</section>


<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Polyphonic Music and Counterpoint</h2>
        <div class="content has-text-justified">
          <img style="width:100%" src="static/images/counter-point.png" alt="auto dynamics method"/>          
        </div>
        <div class="content has-text-justified is-four-fifths">
          <p>
            An excerpt from Bach’s Prelude and Fugue in B Major, WTC I, BWV 868, with dynamics as generated
            by the base model.
          <p>
            Polyphonic music is defined as music with multiple independent melodic lines played at once. The model is
            successfully able to distinguish between the different melodic lines by making certain melodies softer, as seen in the
            excerpt above (the line highlighted in blue is notably softer than the others).
          <p>
            However, some polyphonic compositions such as the fugue are based on a repeated motif that the melodic lines take
            turns performing. It is common practice within these compositions to emphasize the motif whenever it appears.
          <p>
            In fact, the notes highlighted in blue above represent an iteration of its fugue’s motif. Unfortunately, the model did
            not successfully recognize this passage as a motif, possibly due to either lack of context or lack of training data. This
            is a possible area for improvement.   

          <p><br><br>If useful, please cite with the following: </p>
        </div>
        
      </div>
    </div>
  </div>
</section>

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @article{grace4x2024autodynamics,
          title={Auto-Dynamics: Transformer-Generated Interpretations of Piano Music},
          author={Grace Xu},
          year={2024}
        }        
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>
  </body>
  </html>
